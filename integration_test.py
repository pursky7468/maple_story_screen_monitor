import os
import time
import json
import numpy as np
from datetime import datetime
from screen_monitor import ScreenMonitor, convert_to_json_serializable
from roi_selector import ROISelector
from gemini_analyzer import GeminiAnalyzer
from real_time_merger import setup_real_time_merger, log_test_result
from config import *

class IntegrationTester:
    def __init__(self):
        self.test_folder = None
        self.roi_coordinates = None
        self.real_time_merger = None
        
    def setup_test_environment(self, test_runs):
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        # å‰µå»ºæ¸¬è©¦è³‡æ–™å¤¾ï¼Œä½¿ç”¨æ™‚é–“å‘½å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.test_folder = f"integration_test_{timestamp}"
        
        if not os.path.exists(self.test_folder):
            os.makedirs(self.test_folder)
            
        print(f"æ¸¬è©¦è³‡æ–™å¤¾å·²å‰µå»º: {self.test_folder}")
        
        # è¨­ç½®å¯¦æ™‚åˆä½µå™¨
        self.real_time_merger = setup_real_time_merger(self.test_folder)
        print("å¯¦æ™‚çµæœåˆä½µå™¨å·²å•Ÿç”¨")
        
        # å‰µå»ºæ¸¬è©¦æ‘˜è¦æª”æ¡ˆ
        summary_file = os.path.join(self.test_folder, "test_summary.json")
        test_info = {
            "test_start_time": timestamp,
            "total_runs": test_runs,
            "roi_coordinates": None,
            "selling_items": SELLING_ITEMS,
            "results": []
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(convert_to_json_serializable(test_info), f, ensure_ascii=False, indent=2)
            
        return summary_file
    
    def get_roi_selection(self):
        """ç²å–ROIé¸æ“‡"""
        print("\nè«‹é¸æ“‡æ¸¬è©¦ç”¨çš„ROIå€åŸŸ...")
        print("å³å°‡é¡¯ç¤ºå…¨è¢å¹•æˆªåœ–ï¼Œè«‹ç”¨æ»‘é¼ æ‹–æ‹‰é¸æ“‡ç›£æ§å€åŸŸ")
        input("æŒ‰Enteré–‹å§‹é¸æ“‡ROI...")
        
        selector = ROISelector()
        roi_coordinates = selector.select_roi()
        
        if roi_coordinates is None:
            print("æœªé¸æ“‡ROIå€åŸŸï¼Œæ¸¬è©¦çµæŸ")
            return None
            
        print(f"å·²é¸æ“‡ROI: {roi_coordinates}")
        self.roi_coordinates = roi_coordinates
        return roi_coordinates
    
    def get_analyzer_choice(self):
        """ç²å–åˆ†æå™¨é¸æ“‡"""
        print("\nè«‹é¸æ“‡è¦æ¸¬è©¦çš„åˆ†ææ–¹æ³•ï¼š")
        print("1. Gemini AI (éœ€è¦API Keyï¼Œæº–ç¢ºåº¦é«˜)")
        print("2. OCR (æœ¬åœ°è™•ç†ï¼Œé€Ÿåº¦å¿«)")
        
        while True:
            choice = input("è«‹è¼¸å…¥é¸é … (1 æˆ– 2): ").strip()
            if choice == "1":
                return "gemini"
            elif choice == "2":
                return "ocr"
            else:
                print("è«‹è¼¸å…¥ 1 æˆ– 2")
    
    def create_analyzer(self, analyzer_type: str):
        """å‰µå»ºåˆ†æå™¨å¯¦ä¾‹"""
        if analyzer_type == "gemini":
            if GEMINI_API_KEY == "YOUR_GEMINI_API_KEY_HERE":
                print("éŒ¯èª¤ï¼šè«‹å…ˆåœ¨ config.py ä¸­è¨­ç½®æ‚¨çš„ Gemini API Key")
                return None
            try:
                from gemini_analyzer import GeminiAnalyzer
                return GeminiAnalyzer(GEMINI_API_KEY, SELLING_ITEMS)
            except Exception as e:
                print(f"Geminiåˆ†æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
                return None
        
        elif analyzer_type == "ocr":
            try:
                from ocr_analyzer import OCRAnalyzer
                return OCRAnalyzer(SELLING_ITEMS)
            except ImportError as e:
                print(f"âŒ OCRä¾è³´ç¼ºå¤±: {e}")
                print("\nå®‰è£OCRä¾è³´ï¼š")
                print("æ–¹æ³•1 (æ¨è–¦)ï¼špython install_ocr.py")
                print("æ–¹æ³•2 (æ‰‹å‹•)ï¼špip install easyocr")
                print("\næ³¨æ„ï¼šé¦–æ¬¡ä½¿ç”¨OCRæœƒè‡ªå‹•ä¸‹è¼‰èªè¨€æ¨¡å‹ï¼Œéœ€è¦ç¶²è·¯é€£ç·š")
                return None
            except Exception as e:
                print(f"âŒ OCRåˆå§‹åŒ–å¤±æ•—: {e}")
                return None
        
        
        return None
    
    def run_single_test(self, test_id, monitor, fallback_analyzer=None):
        """åŸ·è¡Œå–®æ¬¡æ¸¬è©¦"""
        print(f"\n--- åŸ·è¡Œæ¸¬è©¦ {test_id} ---")
        
        # ä½¿ç”¨çµ±ä¸€çš„æ™‚é–“æˆ³è¨˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # åŒ…å«æ¯«ç§’
        
        # æˆªå–ROIåœ–åƒ
        roi_image = monitor.capture_roi()
        if roi_image is None:
            print(f"æ¸¬è©¦ {test_id}: æˆªåœ–å¤±æ•—")
            return None
            
        # ä¿å­˜æˆªåœ–
        screenshot_path = os.path.join(self.test_folder, f"test_{test_id:03d}_{timestamp}_screenshot.png")
        roi_image.save(screenshot_path)
        
        # é€²è¡Œåˆ†æ
        analysis_start_time = datetime.now()
        result, analysis_result = monitor.analyze_with_strategy(roi_image)
        analysis_end_time = datetime.now()
        
        # ä½¿ç”¨ç­–ç•¥æ¨¡å¼è™•ç†éŒ¯èª¤ï¼ˆå®Œå…¨éš”é›¢çš„éŒ¯èª¤åˆ¤æ–·ï¼‰
        if analysis_result == "ERROR" or str(analysis_result).startswith("ERROR"):
            # è®“åˆ†æå™¨è‡ªå·±æ±ºå®šéŒ¯èª¤é¡å‹
            error_type = monitor.analyzer.get_error_type(str(analysis_result))
            
            # å¦‚æœæ˜¯APIé…é¡éŒ¯èª¤ä¸”æœ‰å‚™ç”¨åˆ†æå™¨ï¼Œè‡ªå‹•åˆ‡æ›
            if error_type == "API_QUOTA_EXCEEDED" and fallback_analyzer:
                print(f"âš ï¸  æ¸¬è©¦ {test_id}: APIé…é¡å·²ç”¨ç›¡ï¼Œåˆ‡æ›åˆ°OCRåˆ†æ")
                
                # ä½¿ç”¨å‚™ç”¨åˆ†æå™¨é‡æ–°åˆ†æ
                fallback_start_time = datetime.now()
                fallback_result, fallback_analysis_result = fallback_analyzer.analyze_image(roi_image), None
                
                # OCRåˆ†æå™¨ç›´æ¥è¿”å›çµæ§‹åŒ–çµæœ
                if not (isinstance(fallback_result, str) and fallback_result.startswith("ERROR")):
                    parsed_fallback = fallback_analyzer.parse_result(fallback_result)
                    fallback_end_time = datetime.now()
                    
                    print(f"âœ… æ¸¬è©¦ {test_id}: å·²åˆ‡æ›åˆ°OCRå®Œæˆåˆ†æ")
                    
                    # ä¿å­˜åˆ†æçµæœï¼ˆæ¨™è¨˜ç‚ºä½¿ç”¨å‚™ç”¨åˆ†æå™¨ï¼‰
                    analysis_path = os.path.join(self.test_folder, f"test_{test_id:03d}_{timestamp}_analysis.json")
                    fallback_test_result = {
                        "test_id": test_id,
                        "timestamp": timestamp,
                        "screenshot_path": screenshot_path,
                        "analysis_start_time": analysis_start_time.isoformat(),
                        "analysis_end_time": fallback_end_time.isoformat(),
                        "analysis_duration_ms": (fallback_end_time - fallback_start_time).total_seconds() * 1000,
                        "primary_analyzer": monitor.analyzer.strategy_type,
                        "fallback_analyzer": fallback_analyzer.strategy_type,
                        "fallback_used": True,
                        "primary_error": str(analysis_result),
                        "parsed_result": parsed_fallback.to_dict()
                    }
                    
                    with open(analysis_path, 'w', encoding='utf-8') as f:
                        json.dump(convert_to_json_serializable(fallback_test_result), f, ensure_ascii=False, indent=2)
                    
                    # é€²è¡ŒåŒ¹é…æª¢æŸ¥
                    is_match = parsed_fallback.is_match
                    match_details = fallback_analyzer.format_match_info(parsed_fallback) if hasattr(fallback_analyzer, 'format_match_info') else ""
                    
                    print(f"æ¸¬è©¦ {test_id} å®Œæˆ (ä½¿ç”¨OCRå‚™ç”¨):")
                    print(f"  æˆªåœ–: {screenshot_path}")
                    print(f"  åˆ†æ: {analysis_path}")
                    print(f"  åŒ¹é…: {'æ˜¯' if is_match else 'å¦'}")
                    if is_match and match_details:
                        print(f"  è©³æƒ…: {match_details[:100]}...")
                    
                    # è¨˜éŒ„åˆ°å¯¦æ™‚åˆä½µå™¨
                    if self.real_time_merger:
                        log_test_result(self.real_time_merger, test_id, screenshot_path, parsed_fallback.to_dict(), None)
                        
                    return {
                        "test_id": test_id,
                        "timestamp": timestamp,
                        "screenshot_path": screenshot_path,
                        "analysis_path": analysis_path,
                        "is_match": is_match,
                        "analysis_duration_ms": (fallback_end_time - fallback_start_time).total_seconds() * 1000,
                        "fallback_used": True,
                        "primary_analyzer": monitor.analyzer.strategy_type,
                        "fallback_analyzer": fallback_analyzer.strategy_type,
                        "success": True
                    }
            
            # æ ¹æ“šéŒ¯èª¤é¡å‹ç”Ÿæˆé©ç•¶çš„éŒ¯èª¤ä¿¡æ¯
            if error_type == "API_QUOTA_EXCEEDED":
                error_message = "APIé…é¡å·²ç”¨ç›¡"
                print(f"âš ï¸  æ¸¬è©¦ {test_id}: {error_message}")
            else:
                error_message = f"{monitor.analyzer.strategy_type}åˆ†æå¤±æ•—"
                print(f"âŒ æ¸¬è©¦ {test_id}: {error_message}")
            
            error_info = {
                "error": error_message,
                "error_type": error_type,
                "strategy_type": monitor.analyzer.strategy_type,
                "raw_response": str(analysis_result),
                "fallback_available": fallback_analyzer is not None,
                "fallback_attempted": error_type == "API_QUOTA_EXCEEDED" and fallback_analyzer is not None
            }
            
            # è¨˜éŒ„åˆ°å¯¦æ™‚åˆä½µå™¨
            if self.real_time_merger:
                log_test_result(self.real_time_merger, test_id, screenshot_path, None, error_info)
            
            return {
                "test_id": test_id,
                "timestamp": timestamp,
                "screenshot_path": screenshot_path,
                "error": error_message,
                "error_type": error_type,
                "strategy_type": monitor.analyzer.strategy_type,
                "analysis_duration_ms": (analysis_end_time - analysis_start_time).total_seconds() * 1000,
                "fallback_available": fallback_analyzer is not None,
                "success": False
            }
        
        # ä¿å­˜åˆ†æçµæœ
        analysis_path = os.path.join(self.test_folder, f"test_{test_id:03d}_{timestamp}_analysis.json")
        
        try:
            # ä½¿ç”¨ç­–ç•¥æ¨¡å¼è™•ç†çµæœè§£æï¼ˆé¿å…ç¡¬ç·¨ç¢¼åˆ¤æ–·ï¼‰
            if monitor.analyzer.strategy_type == "GEMINI":
                json_content = monitor.analyzer.extract_json_from_response(analysis_result)
                parsed_result = json.loads(json_content)
            else:
                # å…¶ä»–åˆ†æå™¨ç›´æ¥è¿”å›çµæ§‹åŒ–çµæœ
                parsed_result = result.to_dict()
            
            # æ·»åŠ æ¸¬è©¦å…ƒæ•¸æ“š
            test_result = {
                "test_id": test_id,
                "timestamp": timestamp,
                "screenshot_path": screenshot_path,
                "analysis_start_time": analysis_start_time.isoformat(),
                "analysis_end_time": analysis_end_time.isoformat(),
                "analysis_duration_ms": (analysis_end_time - analysis_start_time).total_seconds() * 1000,
                "raw_response": analysis_result,
                "parsed_result": parsed_result
            }
            
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(convert_to_json_serializable(test_result), f, ensure_ascii=False, indent=2)
                
        except json.JSONDecodeError as e:
            # ä½¿ç”¨ç­–ç•¥æ¨¡å¼è™•ç†JSONè§£æéŒ¯èª¤
            if monitor.analyzer.strategy_type == "GEMINI":
                extracted_json = monitor.analyzer.extract_json_from_response(analysis_result)
            else:
                extracted_json = str(analysis_result)
            
            error_analysis = {
                "error_type": "JSONè§£æéŒ¯èª¤",
                "error_message": str(e),
                "error_line": getattr(e, 'lineno', None),
                "error_column": getattr(e, 'colno', None),
                "error_position": getattr(e, 'pos', None),
                "raw_response_length": len(analysis_result),
                "extracted_json_length": len(extracted_json),
                "has_json_markers": "```json" in analysis_result,
                "has_closing_markers": "```" in analysis_result and analysis_result.count("```") >= 2,
                "starts_with_brace": extracted_json.strip().startswith('{'),
                "ends_with_brace": extracted_json.strip().endswith('}'),
                "brace_balance": extracted_json.count('{') - extracted_json.count('}'),
                "quote_balance": extracted_json.count('"') % 2 == 0,
                "common_issues": []
            }
            
            # æª¢æŸ¥å¸¸è¦‹å•é¡Œ
            if not extracted_json.strip():
                error_analysis["common_issues"].append("æå–çš„JSONå…§å®¹ç‚ºç©º")
            if "```json" in analysis_result and not extracted_json.strip().startswith('{'):
                error_analysis["common_issues"].append("æ‰¾åˆ°jsonæ¨™è¨˜ä½†å…§å®¹ä¸æ˜¯JSONæ ¼å¼")
            if extracted_json.count('{') != extracted_json.count('}'):
                error_analysis["common_issues"].append(f"å¤§æ‹¬è™Ÿä¸å¹³è¡¡: {{ {extracted_json.count('{')} å€‹, }} {extracted_json.count('}')} å€‹")
            if extracted_json.count('"') % 2 != 0:
                error_analysis["common_issues"].append("å¼•è™Ÿä¸å¹³è¡¡")
            if '\\n' in extracted_json and not extracted_json.replace('\\n', '\n').strip():
                error_analysis["common_issues"].append("å¯èƒ½åŒ…å«æ›è¡Œç¬¦å•é¡Œ")
                
            test_result = {
                "test_id": test_id,
                "timestamp": timestamp,
                "screenshot_path": screenshot_path,
                "analysis_start_time": analysis_start_time.isoformat(),
                "analysis_end_time": analysis_end_time.isoformat(),
                "analysis_duration_ms": (analysis_end_time - analysis_start_time).total_seconds() * 1000,
                "raw_response": analysis_result,
                "extracted_json": extracted_json,
                "error_analysis": error_analysis,
                "parsed_result": None
            }
            
            # ä¿å­˜è©³ç´°éŒ¯èª¤å ±å‘Š
            with open(analysis_path.replace('.json', '_error.json'), 'w', encoding='utf-8') as f:
                json.dump(convert_to_json_serializable(test_result), f, ensure_ascii=False, indent=2)
                
            # ä¹Ÿä¿å­˜ç´”æ–‡å­—ç‰ˆæœ¬æ–¹ä¾¿æŸ¥çœ‹
            with open(analysis_path.replace('.json', '_debug.txt'), 'w', encoding='utf-8') as f:
                f.write(f"=== æ¸¬è©¦ {test_id} JSONè§£æéŒ¯èª¤è©³ç´°å ±å‘Š ===\n\n")
                f.write(f"æ™‚é–“æˆ³: {timestamp}\n")
                f.write(f"éŒ¯èª¤é¡å‹: {error_analysis['error_type']}\n")
                f.write(f"éŒ¯èª¤è¨Šæ¯: {error_analysis['error_message']}\n")
                if error_analysis['error_line']:
                    f.write(f"éŒ¯èª¤è¡Œè™Ÿ: {error_analysis['error_line']}\n")
                if error_analysis['error_column']:
                    f.write(f"éŒ¯èª¤åˆ—è™Ÿ: {error_analysis['error_column']}\n")
                f.write(f"\n=== å…§å®¹åˆ†æ ===\n")
                f.write(f"åŸå§‹å›æ‡‰é•·åº¦: {error_analysis['raw_response_length']} å­—ç¬¦\n")
                f.write(f"æå–JSONé•·åº¦: {error_analysis['extracted_json_length']} å­—ç¬¦\n")
                f.write(f"åŒ…å«```jsonæ¨™è¨˜: {error_analysis['has_json_markers']}\n")
                f.write(f"åŒ…å«çµæŸæ¨™è¨˜: {error_analysis['has_closing_markers']}\n")
                f.write(f"ä»¥{{é–‹å§‹: {error_analysis['starts_with_brace']}\n")
                f.write(f"ä»¥}}çµæŸ: {error_analysis['ends_with_brace']}\n")
                f.write(f"å¤§æ‹¬è™Ÿå¹³è¡¡: {error_analysis['brace_balance']} (0ç‚ºå¹³è¡¡)\n")
                f.write(f"å¼•è™Ÿå¹³è¡¡: {error_analysis['quote_balance']}\n")
                
                if error_analysis['common_issues']:
                    f.write(f"\n=== ç™¼ç¾çš„å•é¡Œ ===\n")
                    for issue in error_analysis['common_issues']:
                        f.write(f"- {issue}\n")
                
                f.write(f"\n=== åŸå§‹å›æ‡‰ ===\n")
                f.write(analysis_result)
                f.write(f"\n\n=== æå–çš„JSON ===\n")
                f.write(extracted_json)
                
                # å¦‚æœJSONå¾ˆçŸ­ï¼Œå˜—è©¦é€å­—ç¬¦åˆ†æ
                if len(extracted_json.strip()) < 500:
                    f.write(f"\n\n=== å­—ç¬¦åˆ†æ ===\n")
                    for i, char in enumerate(extracted_json):
                        f.write(f"{i:3d}: '{char}' (ASCII: {ord(char)})\n")
        
        # é€²è¡ŒåŒ¹é…æª¢æŸ¥ï¼ˆä¸è§¸ç™¼æç¤ºçª—ï¼‰
        is_match = result.is_match
        match_details = monitor.format_match_info(result)
        
        print(f"æ¸¬è©¦ {test_id} å®Œæˆ:")
        print(f"  æˆªåœ–: {screenshot_path}")
        print(f"  åˆ†æ: {analysis_path}")
        print(f"  åŒ¹é…: {'æ˜¯' if is_match else 'å¦'}")
        if is_match:
            print(f"  è©³æƒ…: {match_details[:100]}...")
        
        # è¨˜éŒ„åˆ°å¯¦æ™‚åˆä½µå™¨
        if self.real_time_merger:
            log_test_result(self.real_time_merger, test_id, screenshot_path, result.to_dict() if hasattr(result, 'to_dict') else result, None)
            
        return {
            "test_id": test_id,
            "timestamp": timestamp,
            "screenshot_path": screenshot_path,
            "analysis_path": analysis_path,
            "is_match": is_match,
            "analysis_duration_ms": (analysis_end_time - analysis_start_time).total_seconds() * 1000,
            "success": True
        }
    
    def run_integration_test(self, test_runs):
        """åŸ·è¡Œæ•´åˆæ¸¬è©¦"""
        print(f"\né–‹å§‹æ•´åˆæ¸¬è©¦ - ç¸½å…± {test_runs} æ¬¡")
        
        # è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        summary_file = self.setup_test_environment(test_runs)
        
        # ç²å–ROIé¸æ“‡
        roi_coordinates = self.get_roi_selection()
        if roi_coordinates is None:
            return
        
        # è®“ä½¿ç”¨è€…é¸æ“‡åˆ†æç­–ç•¥
        analyzer_type = self.get_analyzer_choice()
        analyzer = self.create_analyzer(analyzer_type)
        if analyzer is None:
            print("åˆ†æå™¨å‰µå»ºå¤±æ•—ï¼Œæ¸¬è©¦çµæŸ")
            return
        
        # å‰µå»ºå‚™ç”¨åˆ†æå™¨ï¼ˆç•¶ä¸»åˆ†æå™¨æ˜¯Geminiæ™‚ï¼Œä½¿ç”¨OCRä½œç‚ºå‚™ç”¨ï¼‰
        fallback_analyzer = None
        if analyzer_type == "gemini":
            try:
                from ocr_analyzer import OCRAnalyzer
                from config import SELLING_ITEMS
                fallback_analyzer = OCRAnalyzer(SELLING_ITEMS)
                print("âœ… OCRå‚™ç”¨åˆ†æå™¨å·²å‰µå»ºï¼Œå°‡åœ¨APIé…é¡ç”¨ç›¡æ™‚è‡ªå‹•åˆ‡æ›")
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•å‰µå»ºOCRå‚™ç”¨åˆ†æå™¨: {e}")
                print("å°‡ç¹¼çºŒä½¿ç”¨å–®ä¸€åˆ†æå™¨æ¨¡å¼")
        
        # å‰µå»ºç›£æ§å™¨ï¼ˆé—œé–‰æç¤ºçª—åŠŸèƒ½ï¼‰
        monitor = ScreenMonitor(roi_coordinates, analyzer, save_screenshots=False, show_alerts=False)
        
        # æ›´æ–°æ¸¬è©¦æ‘˜è¦ä¸­çš„ROIè³‡è¨Šå’Œåˆ†ææ–¹æ³•
        with open(summary_file, 'r', encoding='utf-8') as f:
            summary = json.load(f)
        summary["roi_coordinates"] = roi_coordinates
        summary["analyzer_type"] = analyzer_type
        summary["analyzer_class"] = analyzer.__class__.__name__
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(convert_to_json_serializable(summary), f, ensure_ascii=False, indent=2)
        
        # åŸ·è¡Œæ¸¬è©¦å¾ªç’°
        results = []
        match_count = 0
        quota_exceeded_count = 0
        
        for i in range(1, test_runs + 1):
            try:
                result = self.run_single_test(i, monitor, fallback_analyzer)
                if result:
                    results.append(result)
                    
                    # æª¢æŸ¥åŒ¹é…çµæœ
                    if result.get("success", False) and result.get("is_match", False):
                        match_count += 1
                    
                    # æª¢æŸ¥é…é¡éŒ¯èª¤
                    if result.get("error_type") == "API_QUOTA_EXCEEDED":
                        quota_exceeded_count += 1
                        
                        # å¦‚æœé€£çºŒ3æ¬¡é…é¡éŒ¯èª¤ï¼Œè©¢å•æ˜¯å¦ç¹¼çºŒ
                        if quota_exceeded_count >= 3:
                            print(f"\nâš ï¸  é€£çºŒé‡åˆ°APIé…é¡éŒ¯èª¤")
                            print("å»ºè­°ï¼š")
                            print("1. ç­‰å¾…24å°æ™‚é…é¡é‡ç½®")
                            print("2. å‡ç´šåˆ°ä»˜è²»è¨ˆç•«")
                            print("3. ä½¿ç”¨ä¸åŒçš„API Key")
                            
                            choice = input("æ˜¯å¦ç¹¼çºŒæ¸¬è©¦ï¼Ÿ(y/n): ").lower().strip()
                            if choice not in ['y', 'yes', 'æ˜¯']:
                                print(f"æ¸¬è©¦åœ¨ç¬¬ {i} æ¬¡å¾Œåœæ­¢ï¼ˆé…é¡é™åˆ¶ï¼‰")
                                break
                            else:
                                quota_exceeded_count = 0  # é‡ç½®è¨ˆæ•¸å™¨
                    else:
                        quota_exceeded_count = 0  # é‡ç½®é…é¡éŒ¯èª¤è¨ˆæ•¸å™¨
                
                # æ¯10æ¬¡æ¸¬è©¦é¡¯ç¤ºé€²åº¦
                if i % 10 == 0:
                    print(f"\né€²åº¦: {i}/{test_runs} ({i/test_runs*100:.1f}%)")
                    print(f"æˆåŠŸæ¸¬è©¦: {len([r for r in results if r.get('success', False)])}")
                    print(f"ç›®å‰åŒ¹é…: {match_count}")
                
                # æ¸¬è©¦é–“éš”ï¼ˆé¿å…APIé™åˆ¶ï¼‰
                if i < test_runs:
                    time.sleep(SCAN_INTERVAL)
                    
            except KeyboardInterrupt:
                print(f"\næ¸¬è©¦è¢«ä¸­æ–·åœ¨ç¬¬ {i} æ¬¡")
                break
            except Exception as e:
                print(f"æ¸¬è©¦ {i} ç™¼ç”ŸéŒ¯èª¤: {e}")
                results.append({
                    "test_id": i,
                    "error": str(e),
                    "error_type": "UNKNOWN_ERROR",
                    "success": False
                })
        
        # åˆ†ææ¸¬è©¦çµæœ
        completed_tests = [r for r in results if r.get("success", False)]
        failed_tests = [r for r in results if not r.get("success", False)]
        fallback_used_tests = [r for r in completed_tests if r.get("fallback_used", False)]
        
        # çµ±è¨ˆéŒ¯èª¤é¡å‹
        error_stats = {}
        for result in failed_tests:
            error_type = result.get("error", "æœªçŸ¥éŒ¯èª¤")
            error_stats[error_type] = error_stats.get(error_type, 0) + 1
        
        # çµ±è¨ˆåˆ†ææ™‚é–“
        analysis_times = [r.get("analysis_duration_ms", 0) for r in completed_tests]
        avg_analysis_time = sum(analysis_times) / len(analysis_times) if analysis_times else 0
        
        # æ›´æ–°æœ€çµ‚æ¸¬è©¦æ‘˜è¦
        summary["results"] = results
        summary["test_end_time"] = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary["statistics"] = {
            "total_completed": len(completed_tests),
            "total_failed": len(failed_tests),
            "success_rate": f"{len(completed_tests)/len(results)*100:.1f}%" if results else "0%",
            "total_matches": match_count,
            "match_rate": f"{match_count/len(completed_tests)*100:.1f}%" if completed_tests else "0%",
            "fallback_used_count": len(fallback_used_tests),
            "fallback_usage_rate": f"{len(fallback_used_tests)/len(results)*100:.1f}%" if results else "0%",
            "average_analysis_time_ms": round(avg_analysis_time, 2),
            "min_analysis_time_ms": min(analysis_times) if analysis_times else 0,
            "max_analysis_time_ms": max(analysis_times) if analysis_times else 0,
            "error_breakdown": error_stats,
            "has_fallback_analyzer": fallback_analyzer is not None
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(convert_to_json_serializable(summary), f, ensure_ascii=False, indent=2)
        
        # é¡¯ç¤ºè©³ç´°æ¸¬è©¦çµæœ
        stats = summary["statistics"]
        print(f"\n{'='*60}")
        print("æ•´åˆæ¸¬è©¦å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"æ¸¬è©¦è³‡æ–™å¤¾: {self.test_folder}")
        print(f"åˆ†ææ–¹æ³•: {summary.get('analyzer_class', 'æœªçŸ¥')} ({summary.get('analyzer_type', 'æœªçŸ¥')})")
        print(f"åŸ·è¡Œçµæœ: {len(results)}/{test_runs} æ¬¡æ¸¬è©¦")
        print(f"æˆåŠŸç‡: {stats['success_rate']}")
        print(f"å¤±æ•—æ¬¡æ•¸: {stats['total_failed']}")
        print(f"åŒ¹é…æ¬¡æ•¸: {stats['total_matches']}")
        print(f"åŒ¹é…ç‡: {stats['match_rate']}")
        if stats.get('has_fallback_analyzer', False):
            print(f"OCRå‚™ç”¨ä½¿ç”¨: {stats['fallback_used_count']} æ¬¡ ({stats['fallback_usage_rate']})")
        print(f"å¹³å‡åˆ†ææ™‚é–“: {stats['average_analysis_time_ms']} æ¯«ç§’")
        print(f"åˆ†ææ™‚é–“ç¯„åœ: {stats['min_analysis_time_ms']}-{stats['max_analysis_time_ms']} æ¯«ç§’")
        
        if stats['error_breakdown']:
            print(f"\néŒ¯èª¤çµ±è¨ˆ:")
            for error_type, count in stats['error_breakdown'].items():
                print(f"  - {error_type}: {count} æ¬¡")
        
        # ç”Ÿæˆæœ€çµ‚çš„åˆä½µå ±å‘Š
        if self.real_time_merger:
            self.real_time_merger.generate_quick_html()
            print(f"\nğŸ¯ åˆä½µå ±å‘Šå·²ç”Ÿæˆ:")
            print(f"  - quick_view.html: å¿«é€ŸæŸ¥çœ‹å™¨ï¼ˆæ¨è–¦ï¼‰")
            print(f"  - combined_results.json: åˆä½µçš„JSONæ•¸æ“š")
        
        print(f"\nğŸ“ æª”æ¡ˆèªªæ˜:")
        print(f"  - test_summary.json: å®Œæ•´æ¸¬è©¦æ‘˜è¦")
        print(f"  - quick_view.html: åœ–ç‰‡+çµæœåˆä½µæŸ¥çœ‹å™¨ ğŸŒŸ")
        print(f"  - combined_results.json: åˆä½µçš„æ¸¬è©¦æ•¸æ“š")
        print(f"  - *_screenshot.png: æ¸¬è©¦æˆªåœ–")
        print(f"  - *_analysis.json: æˆåŠŸè§£æçš„çµæœ")
        print(f"  - *_error.json: JSONè§£æéŒ¯èª¤çš„è©³ç´°åˆ†æ")
        print(f"  - *_debug.txt: äººé¡å¯è®€çš„éŒ¯èª¤åˆ†æå ±å‘Š")
        print(f"{'='*60}")
        
        if self.real_time_merger and len(self.real_time_merger.merged_results) > 0:
            print(f"ğŸ’¡ èª¿è©¦å»ºè­°ï¼š")
            print(f"   1. æ‰“é–‹ {self.test_folder}/quick_view.html æŸ¥çœ‹æ¸¬è©¦çµæœ")
            print(f"   2. é»æ“Šåœ–ç‰‡å¯ä»¥æ”¾å¤§æŸ¥çœ‹")
            print(f"   3. åˆä½µå ±å‘ŠåŒ…å«äº†æˆªåœ–å’Œåˆ†æçµæœï¼Œä¾¿æ–¼èª¿è©¦")
            print(f"{'='*60}")

def main():
    """ä¸»ç¨‹å¼"""
    print("è¢å¹•ç›£æ§æ•´åˆæ¸¬è©¦ç¨‹å¼")
    print("=" * 40)
    print("æ”¯æ´çš„åˆ†ææ–¹æ³•ï¼š")
    print("- Gemini AI (éœ€è¦è¨­ç½®API Key)")
    print("- OCR æœ¬åœ°è­˜åˆ¥ (éœ€è¦å®‰è£easyocr)")
    print("=" * 40)
    
    # ç²å–æ¸¬è©¦æ¬¡æ•¸
    while True:
        try:
            test_runs = int(input("è«‹è¼¸å…¥æ¸¬è©¦æ¬¡æ•¸ (1-1000): "))
            if 1 <= test_runs <= 1000:
                break
            else:
                print("è«‹è¼¸å…¥1åˆ°1000ä¹‹é–“çš„æ•¸å­—")
        except ValueError:
            print("è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
    
    # åŸ·è¡Œæ•´åˆæ¸¬è©¦
    tester = IntegrationTester()
    tester.run_integration_test(test_runs)

if __name__ == "__main__":
    main()